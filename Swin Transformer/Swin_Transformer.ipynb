{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Swin_Transformer.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "_0qlip1k_sjI"
      },
      "source": [
        "####"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "I9SgSJ6U_xtz"
      },
      "source": [
        "import torch\n",
        "from torch import nn"
      ],
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VewOWrnZ_692"
      },
      "source": [
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "13LnkWzt_0Uj"
      },
      "source": [
        "def window_partition(x, window_size):\n",
        "    B, H, W, C = x.shape\n",
        "    x = x.view(B, H // window_size, window_size, W // window_size, window_size, C)\n",
        "    windows = x.permute(0, 1, 3, 2, 4, 5).contiguous().view(-1, window_size, window_size, C)\n",
        "    return windows"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LsApk0P9_3dq"
      },
      "source": [
        "x = torch.randn(2 , 224 , 224 , 3).to(device)\n",
        "window_size = 4\n",
        "z = window_partition(x , window_size)\n",
        "z.shape"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "upQ7tDJHAHA7"
      },
      "source": [
        "def window_reverse(windows, window_size, H, W):\n",
        "    B = int(windows.shape[0] / (H * W / window_size / window_size))\n",
        "    x = windows.view(B, H // window_size, W // window_size, window_size, window_size, -1)\n",
        "    x = x.permute(0, 1, 3, 2, 4, 5).contiguous().view(B, H, W, -1)\n",
        "    return x"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WxVD3JaVAf9q"
      },
      "source": [
        "y = window_reverse(z , 4 , 224 , 224)\n",
        "y.shape"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WtloS9HiAl5K"
      },
      "source": [
        "class MLP(nn.Module):\n",
        "    def __init__(self , \n",
        "                 in_channels , \n",
        "                 hidden_dim = None, \n",
        "                 out_channels = None , \n",
        "                 use_activation = True , \n",
        "                 dropout = 0.0):\n",
        "        super(MLP , self).__init__()\n",
        "        \n",
        "        self.use_activation = use_activation\n",
        "        out_channels = out_channels or in_channels\n",
        "        hidden_dim = hidden_dim or in_channels\n",
        "\n",
        "        self.fc1 = nn.Linear(in_channels , hidden_dim)\n",
        "        if self.use_activation:\n",
        "            self.activation = nn.GELU()\n",
        "        self.fc2 = nn.Linear(hidden_dim , out_channels)\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "    def forward(self , x):\n",
        "        x = self.fc1(x)\n",
        "        x = self.activation(x)\n",
        "        x = self.dropout(x)\n",
        "        x = self.fc2(x)\n",
        "        x = self.dropout(x)\n",
        "        return x"
      ],
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Lo_oswZgA5pp"
      },
      "source": [
        "class Window_Attention(nn.Module):\n",
        "    def __init__(self , \n",
        "                 dim , \n",
        "                 window_size , \n",
        "                 num_heads , \n",
        "                 qk_scale = None , \n",
        "                 attn_dropout = 0.0 , \n",
        "                 proj_drop = 0.0):\n",
        "        super(Window_Attention , self).__init__()\n",
        "\n",
        "        self.dim = dim\n",
        "        self.window_size = window_size\n",
        "        self.num_heads = num_heads\n",
        "        head_dim = dim // num_heads\n",
        "        self.scale = qk_scale  or head_dim ** -0.5\n",
        "\n",
        "\n",
        "\n",
        "        self.qkv = nn.Linear(dim , dim * 3 , bias=False)\n",
        "        self.attn_dropout = nn.Dropout(attn_dropout)\n",
        "        self.proj = nn.Linear(dim , dim)\n",
        "        self.proj_dropout = nn.Dropout(proj_drop)\n",
        "        self.softmax = nn.Softmax(-1)\n",
        "\n",
        "    \n",
        "    def forward(self , x , mask = None):\n",
        "        B_ , N , C = x.shape\n",
        "        qkv = self.qkv(x).reshape(B_ , N , 3 , self.num_heads , C//self.num_heads).permute(2 , 0 , 3 , 1 , 4)\n",
        "        q , k , v = qkv[0] , qkv[1] , qkv[2]\n",
        "        q = q * self.scale\n",
        "\n",
        "        attn = (q @ k.transpose(-2 , -1))\n",
        "\n",
        "        if mask is not None:\n",
        "            nW = mask.shape[0]\n",
        "            attn = attn.view(B_ // nW , nW , self.num_heads , N , N)\n",
        "            attn = attn.view(-1 , self.num_heads , N , N)\n",
        "            attn = self.softmax(attn)\n",
        "        else :\n",
        "            attn = self.softmax(attn)\n",
        "        attn = self.attn_dropout(attn)\n",
        "\n",
        "        x = (attn @ v).transpose(1 , 2).reshape(B_ , N , C)\n",
        "        x = self.proj(x)\n",
        "        x = self.proj_dropout(x)\n",
        "        return x\n"
      ],
      "execution_count": 100,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EFX7UZ4zLiWK"
      },
      "source": [
        "class Swin_Transformer_Block(nn.Module):\n",
        "    def __init__(self , \n",
        "                 in_channels , \n",
        "                 input_res , \n",
        "                 num_heads , \n",
        "                 window_size = 7 , \n",
        "                 shift_size = 0 , \n",
        "                 mlp_ratio = 4.0 , \n",
        "                 qk_scale = None , \n",
        "                 drop = 0.0 , \n",
        "                 attn_drop = 0.0 ):\n",
        "        super(Swin_Transformer_Block , self).__init__()\n",
        "\n",
        "        self.in_channels = in_channels\n",
        "        self.input_res = input_res\n",
        "        self.num_heads = num_heads\n",
        "        self.window_size = window_size\n",
        "        self.shift_size = shift_size\n",
        "        self.mlp_ratio = mlp_ratio\n",
        "        self.qk_scale = qk_scale\n",
        "        \n",
        "        if min(self.input_res) <= self.window_size:\n",
        "            self.shidt_size = 0\n",
        "            self.window_size = min(self.input_res)\n",
        "        \n",
        "        self.norm1 = nn.LayerNorm(in_channels)\n",
        "        self.attn = Window_Attention(\n",
        "            self.in_channels , self.window_size , self.num_heads , self.qk_scale , attn_drop , drop\n",
        "        )\n",
        "\n",
        "        self.dropout_path = nn.Identity()\n",
        "        self.norm2 = nn.LayerNorm(in_channels)\n",
        "        mlp_hidden_dim = int(in_channels * self.mlp_ratio)\n",
        "        self.mlp = MLP(self.in_channels , mlp_hidden_dim , dropout = drop)\n",
        "\n",
        "        if self.shift_size > 0:\n",
        "           \n",
        "            H, W = self.input_res\n",
        "            img_mask = torch.zeros((1, H, W, 1)) \n",
        "            h_slices = (slice(0, -self.window_size),\n",
        "                        slice(-self.window_size, -self.shift_size),\n",
        "                        slice(-self.shift_size, None))\n",
        "            w_slices = (slice(0, -self.window_size),\n",
        "                        slice(-self.window_size, -self.shift_size),\n",
        "                        slice(-self.shift_size, None))\n",
        "            cnt = 0\n",
        "            for h in h_slices:\n",
        "                for w in w_slices:\n",
        "                    img_mask[:, h, w, :] = cnt\n",
        "                    cnt += 1\n",
        "        \n",
        "            mask_windows = window_partition(img_mask, self.window_size)  # nW, window_size, window_size, 1\n",
        "            mask_windows = mask_windows.view(-1, self.window_size * self.window_size)\n",
        "            attn_mask = mask_windows.unsqueeze(1) - mask_windows.unsqueeze(2)\n",
        "            self.attn_mask = attn_mask.masked_fill(attn_mask != 0, float(-100.0)).masked_fill(attn_mask == 0, float(0.0))\n",
        "        else:\n",
        "            self.attn_mask = None\n",
        "\n",
        "    def forward(self , x):\n",
        "        H , W = self.input_res\n",
        "        B , L , C = x.shape\n",
        "        assert L == H * W , \"input feature has wrong size\"\n",
        "\n",
        "        x_ = x.clone()\n",
        "        x = self.norm1(x)\n",
        "        x = x.view(B , H , W , C)\n",
        "\n",
        "        if self.shift_size > 0:\n",
        "            shifted_x = torch.roll(x , shifts=(-self.shift_size , -self.shift_size) , dims=(1 , 2))\n",
        "        else :\n",
        "            shifted_x = x\n",
        "        x_window = window_partition(shifted_x , self.window_size)\n",
        "        x_window = x_window.view(-1 , self.window_size * self.window_size , C)\n",
        "\n",
        "        attn_window = self.attn(x_window , mask = self.attn_mask) #\n",
        "        shifted_x = window_reverse(attn_window , self.window_size , H , W)\n",
        "\n",
        "        if self.shift_size > 0:\n",
        "            x = torch.roll(shifted_x , (self.shift_size , self.shift_size) , dims=(1 , 2))\n",
        "        else:\n",
        "            x = shifted_x\n",
        "        x = x.view(B , H * W , C)\n",
        "        x = x_ + self.dropout_path(x)\n",
        "        x = x + self.dropout_path(self.mlp(self.norm2(x)))\n",
        "        return x\n"
      ],
      "execution_count": 115,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8Ai2BThBLo_8"
      },
      "source": [
        "class Patch_Merging(nn.Module):\n",
        "    def __init__(self , \n",
        "                 input_res , \n",
        "                 in_channels):\n",
        "        super(Patch_Merging , self).__init__()\n",
        "\n",
        "        self.input_res = input_res\n",
        "        self.in_channels = in_channels\n",
        "        self.reduction = nn.Linear(4 * in_channels , 2 * in_channels , bias=False)\n",
        "        self.norm = nn.LayerNorm(4 * in_channels)\n",
        "    \n",
        "    def forward(self , x):\n",
        "        H , W = self.input_res\n",
        "        B , L , C = x.shape\n",
        "        assert L == H * W , \"input feature has wrong size\"\n",
        "        assert H % 2 == 0 and W % 2 == 0 , f'x size {H} {W} are not even'\n",
        "        x = x.view(B, H, W, C)\n",
        "\n",
        "        x0 = x[:, 0::2, 0::2, :]  \n",
        "        x1 = x[:, 1::2, 0::2, :]  \n",
        "        x2 = x[:, 0::2, 1::2, :]  \n",
        "        x3 = x[:, 1::2, 1::2, :]  \n",
        "        x = torch.cat([x0, x1, x2, x3], -1)  \n",
        "        x = x.view(B, -1, 4 * C)  \n",
        "\n",
        "        x = self.norm(x)\n",
        "        x = self.reduction(x)\n",
        "\n",
        "        return x"
      ],
      "execution_count": 116,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EAiiQFTYbkpX"
      },
      "source": [
        "class Layer(nn.Module):\n",
        "    def __init__(self , \n",
        "                 in_channels , \n",
        "                 input_res , \n",
        "                 depth , \n",
        "                 num_heads , \n",
        "                 window_size ,\n",
        "                 mlp_ratio = 4.0 , \n",
        "                 qk_scale = None , \n",
        "                 drop = 0.0 , \n",
        "                 attn_drop = 0.0 , \n",
        "                 downsample = None ):\n",
        "        super(Layer , self).__init__()\n",
        "\n",
        "        self.in_channels = in_channels\n",
        "        self.input_res = input_res\n",
        "        self.depth = depth\n",
        "        \n",
        "        self.blocks = nn.ModuleList([\n",
        "                                     Swin_Transformer_Block(\n",
        "                                         self.in_channels , \n",
        "                                         self.input_res ,\n",
        "                                         num_heads , \n",
        "                                         window_size , \n",
        "                                         shift_size = 0 if (i % 2 == 0) else window_size //2 , \n",
        "                                         mlp_ratio = mlp_ratio , \n",
        "                                         qk_scale = qk_scale , \n",
        "                                         drop = drop , \n",
        "                                         attn_drop = attn_drop\n",
        "                                     )\n",
        "        for i in range(depth)])\n",
        "\n",
        "        if downsample is not None:\n",
        "            self.downsample = downsample(self.input_res , in_channels)\n",
        "        else :\n",
        "            self.downsample = None\n",
        "\n",
        "    def forward(self , x):\n",
        "        for block in self.blocks:\n",
        "            x = block(x)\n",
        "        if self.downsample is not None:\n",
        "            x = self.downsample(x)\n",
        "        return x\n"
      ],
      "execution_count": 121,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nxu3vPXzdbcb"
      },
      "source": [
        "class Patch_Embedding(nn.Module):\n",
        "    def __init__(self ,\n",
        "                 img_size = 224 , \n",
        "                 patch_size = 4 , \n",
        "                 in_channels = 3 , \n",
        "                 embed_dim = 96 , \n",
        "                 norm_layer = None):\n",
        "        super(Patch_Embedding , self).__init__()\n",
        "\n",
        "        img_size = (img_size , img_size)\n",
        "        patch_size = (patch_size , patch_size)\n",
        "        patch_res = [img_size[0] // patch_size[0] , img_size[1] // patch_size[1]]\n",
        "\n",
        "        self.img_size = img_size\n",
        "        self.patch_size = patch_size\n",
        "        self.patch_res = patch_res\n",
        "        self.num_patches = patch_res[0] * patch_res[1]\n",
        "\n",
        "        self.in_channels  = in_channels\n",
        "        self.embed_dim = embed_dim\n",
        "\n",
        "        self.proj = nn.Conv2d(self.in_channels , self.embed_dim , kernel_size=self.patch_size , stride=self.patch_size)\n",
        "\n",
        "        if norm_layer is not None:\n",
        "            self.norm = norm_layer(self.embed_dim)\n",
        "        else:\n",
        "            self.norm = None\n",
        "    \n",
        "    def forward(self , x):\n",
        "        B , C , H , W = x.shape\n",
        "        assert H == self.img_size[0] and W == self.img_size[1], \\\n",
        "            f\"Input image size ({H}*{W}) doesn't match model ({self.img_size[0]}*{self.img_size[1]}).\"\n",
        "        x = self.proj(x).flatten(2).transpose(1 , 2)\n",
        "        if self.norm is not None:\n",
        "            x = self.norm(x)\n",
        "        return x"
      ],
      "execution_count": 122,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ak-8G_5re5TE"
      },
      "source": [
        "class Swin_Transformer(nn.Module):\n",
        "    def __init__(self , \n",
        "                 img_size = 224 , \n",
        "                 patch_size = 4 , \n",
        "                 in_channels = 3 , \n",
        "                 num_classes = 1000 , \n",
        "                 embed_dim = 96 , \n",
        "                 depths = [2 , 2 , 6 , 2] , \n",
        "                 num_heads = [3 , 6 , 12 , 24] , \n",
        "                 window_size = 7 , \n",
        "                 mlp_ratio = 4.0 , \n",
        "                 qk_scale = None , \n",
        "                 drop_rate = 0.0 , \n",
        "                 attn_drop_rate = 0.0 , \n",
        "                 norm_layer = nn.LayerNorm , \n",
        "                 patch_norm = True):\n",
        "        super(Swin_Transformer , self).__init__()\n",
        "\n",
        "        self.num_classes = num_classes\n",
        "        self.num_layers = len(depths)\n",
        "        self.embed_dim = embed_dim\n",
        "        self.patch_norm = patch_norm\n",
        "        self.num_features = int(embed_dim * 2 ** (self.num_layers - 1))\n",
        "        self.mlp_ratio = mlp_ratio\n",
        "\n",
        "        self.patch_embed = Patch_Embedding(\n",
        "            img_size = img_size , \n",
        "            patch_size = patch_size , \n",
        "            in_channels = in_channels , \n",
        "            embed_dim = embed_dim , \n",
        "            norm_layer = norm_layer\n",
        "        )\n",
        "\n",
        "        num_patches = self.patch_embed.num_patches\n",
        "        patch_res = self.patch_embed.patch_res\n",
        "        self.patch_res = patch_res\n",
        "\n",
        "        #dpr = [x.item() for x in torch.linspace(0, drop_path_rate, sum(depths))]\n",
        "\n",
        "        self.layer = nn.ModuleList()\n",
        "\n",
        "        for i in range(self.num_layers):\n",
        "            layer = Layer(\n",
        "                in_channels = int(embed_dim * 2 ** i) , \n",
        "                input_res = (patch_res[0] // ( 2 ** i) , \n",
        "                             patch_res[1] // (2 ** i)) , \n",
        "                depth = depths[i] , \n",
        "                num_heads = num_heads[i] , \n",
        "                window_size = window_size , \n",
        "                mlp_ratio = self.mlp_ratio , \n",
        "                qk_scale = qk_scale , \n",
        "                drop = drop_rate , \n",
        "                downsample = Patch_Merging if (i < self.num_layers - 1) else None \n",
        "            )\n",
        "\n",
        "            self.layer.append(layer)\n",
        "        self.norm = norm_layer(self.num_features)\n",
        "        self.avgpool = nn.AdaptiveAvgPool1d(1)\n",
        "        self.head = nn.Linear(self.num_features , num_classes)\n",
        "\n",
        "    def forward_features(self, x):\n",
        "        x = self.patch_embed(x)\n",
        "        for layer in self.layer:\n",
        "            x = layer(x)\n",
        "\n",
        "        x = self.norm(x) \n",
        "        x = self.avgpool(x.transpose(1, 2)) \n",
        "        x = torch.flatten(x, 1)\n",
        "        return x\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.forward_features(x)\n",
        "        x = self.head(x)\n",
        "        return x"
      ],
      "execution_count": 125,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "S6MZvNulhwuu"
      },
      "source": [
        "x = torch.randn(2 , 3 , 224 , 224).to(device)\n",
        "swin_transformer = Swin_Transformer().to(device)\n",
        "z = swin_transformer(x)\n",
        "z.shape"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nVmFeIwDh7j5"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}