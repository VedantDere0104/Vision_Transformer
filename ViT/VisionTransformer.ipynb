{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "VisionTransformer.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "L8J8tyePy1hI"
      },
      "source": [
        "####"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AdnCRszNzG6C"
      },
      "source": [
        "! pip install einops"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2PBattnWy4Nv"
      },
      "source": [
        "import torch\n",
        "from torch import nn\n",
        "from einops import rearrange , reduce , repeat\n",
        "from einops.layers.torch import Rearrange , Reduce"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "S1UaCjPj1BHd"
      },
      "source": [
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ESrOKmQm0FSC"
      },
      "source": [
        "class Conv(nn.Module):\n",
        "    def __init__(self , \n",
        "                 in_channels , \n",
        "                 out_channels , \n",
        "                 kernel_size = (3 , 3) , \n",
        "                 stride = (1 , 1) , \n",
        "                 padding = 1 , \n",
        "                 use_norm = False , \n",
        "                 use_activation = False):\n",
        "        super(Conv , self).__init__()\n",
        "\n",
        "        self.use_norm = use_norm\n",
        "        self.use_activation = use_activation\n",
        "        self.conv1 = nn.Conv2d(in_channels , \n",
        "                               out_channels , \n",
        "                               kernel_size , \n",
        "                               stride , \n",
        "                               padding)\n",
        "        if self.use_norm:\n",
        "            self.norm = nn.InstanceNorm2d(out_channels)\n",
        "        if self.use_activation:\n",
        "            self.activation = nn.LeakyReLU(0.2)\n",
        "    \n",
        "    def forward(self , x):\n",
        "        x = self.conv1(x)\n",
        "        if self.use_activation:\n",
        "            x = self.activation(x)\n",
        "        if self.use_norm:\n",
        "            x = self.norm(x)\n",
        "        return x"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8RZkBosRy5UM"
      },
      "source": [
        "class Patch_Embedding(nn.Module):\n",
        "    def __init__(self , \n",
        "                 in_channels = 3 , \n",
        "                 patch_size = 16 , \n",
        "                 embed_dim = 768 , \n",
        "                 img_size = 224):\n",
        "        super(Patch_Embedding , self).__init__()\n",
        "\n",
        "        self.conv1 = Conv(in_channels , \n",
        "                          embed_dim , \n",
        "                          patch_size , \n",
        "                          patch_size)\n",
        "        self.cls_token = nn.Parameter(torch.randn(1 , 1 , embed_dim))\n",
        "        self.position = nn.Parameter(torch.randn((img_size // patch_size)**2 + 1 , embed_dim))\n",
        "        \n",
        "    def forward(self , x):\n",
        "        batch_size = x.shape[0]\n",
        "        x = self.conv1(x)\n",
        "        x = x.view(x.shape[0] , x.shape[1] , x.shape[2]*x.shape[3]).permute(0 , 2 , 1)\n",
        "        cls_token = repeat(self.cls_token, '() n e -> b n e', b=batch_size)\n",
        "        x = torch.cat([x , cls_token] , dim=1)\n",
        "        x += self.position\n",
        "        return x"
      ],
      "execution_count": 64,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mWhpuDbQ09W_"
      },
      "source": [
        "x = torch.randn(2 , 3 , 224 , 224).to(device)\n",
        "patch_embedding = Patch_Embedding().to(device)\n",
        "z = patch_embedding(x)\n",
        "z.shape"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FXoTCDiZ47c1"
      },
      "source": [
        "class Multi_Head_Attention(nn.Module):\n",
        "    def __init__(self ,\n",
        "                 embed_dim = 768 , \n",
        "                 num_heads = 8 , \n",
        "                 attn_dropout = 0):\n",
        "        super(Multi_Head_Attention , self).__init__()\n",
        "        \n",
        "        self.num_heads = num_heads\n",
        "        self.qkv = nn.Linear(embed_dim , embed_dim * 3)\n",
        "        self.dropout = nn.Dropout(attn_dropout)\n",
        "        self.linear = nn.Linear(embed_dim , embed_dim)\n",
        "        self.embed_dim = embed_dim\n",
        "\n",
        "    def forward(self , x , mask = None):\n",
        "        x = rearrange(self.qkv(x), \"b n (h d qkv) -> (qkv) b h n d\", h=self.num_heads, qkv=3)\n",
        "        queries , keys , values = x\n",
        "        energy = torch.einsum('bhqd, bhkd -> bhqk', queries, keys)\n",
        "        if mask is not None:\n",
        "            fill_value = torch.finfo(torch.float32).min\n",
        "            energy.mask_fill(~mask, fill_value)\n",
        "        scaling = (self.embed_dim) ** (1/2)\n",
        "        attn = torch.nn.functional.softmax(energy , dim=-1)/scaling\n",
        "        attn = self.dropout(attn)\n",
        "        out = torch.einsum('bhal, bhlv -> bhav ', attn, values)\n",
        "        out = rearrange(out, \"b h n d -> b n (h d)\")\n",
        "        out = self.linear(out)\n",
        "        return out"
      ],
      "execution_count": 97,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aBOUCK-Z9hMO"
      },
      "source": [
        "x = torch.randn(2 , 197 , 768).to(device)\n",
        "attn = Multi_Head_Attention().to(device)\n",
        "z = attn(x)\n",
        "z.shape"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IyUIlvrV9rV1"
      },
      "source": [
        "class Residual(nn.Module):\n",
        "    def __init__(self , \n",
        "                 module):\n",
        "        super(Residual , self).__init__()\n",
        "\n",
        "        self.module = module\n",
        "\n",
        "    def forward(self , x , **kwargs):\n",
        "        x_ = x.clone()\n",
        "        x = self.module(x , **kwargs)\n",
        "        x += x_\n",
        "        return x\n"
      ],
      "execution_count": 99,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-wv9gm1CBFnH"
      },
      "source": [
        "class FeedForward(nn.Module):\n",
        "    def __init__(self , \n",
        "                 embed_dim = 768 , \n",
        "                 exp = 4 , \n",
        "                 dropout = 0):\n",
        "        super(FeedForward , self).__init__()\n",
        "\n",
        "        self.linear1 = nn.Linear(embed_dim , embed_dim * exp)\n",
        "        self.gelu = nn.GELU()\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "        self.linear2 = nn.Linear(embed_dim * exp , embed_dim)\n",
        "\n",
        "    def forward(self , x):\n",
        "        x = self.linear1(x)\n",
        "        x = self.gelu(x)\n",
        "        x = self.dropout(x)\n",
        "        x = self.linear2(x)\n",
        "        return x"
      ],
      "execution_count": 110,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VLld7zz2B58u"
      },
      "source": [
        "class Transformer(nn.Module):\n",
        "    def __init__(self , \n",
        "                 embed_dim = 768 , \n",
        "                 dropout = 0):\n",
        "        super(Transformer , self).__init__()\n",
        "\n",
        "        self.norm1 = nn.LayerNorm(embed_dim)\n",
        "        self.multi_attn = Multi_Head_Attention()\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "        self.norm2 = nn.LayerNorm(embed_dim)\n",
        "        self.mlp = FeedForward()\n",
        "\n",
        "    def forward(self , x):\n",
        "        x_ = x.clone()\n",
        "        x = self.norm1(x)\n",
        "        x = self.multi_attn(x)\n",
        "        x = self.dropout(x)\n",
        "        x += x_\n",
        "\n",
        "        x_ = x.clone()\n",
        "        x = self.norm2(x)\n",
        "        x = self.mlp(x)\n",
        "        x = self.dropout(x)\n",
        "        x += x_\n",
        "        return x"
      ],
      "execution_count": 111,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Gq0xylvBCxj4"
      },
      "source": [
        "class Transformer_Encoder(nn.Module):\n",
        "    def __init__(self , \n",
        "                 embed_dim = 768 , \n",
        "                 depth = 12):\n",
        "        super(Transformer_Encoder , self).__init__()\n",
        "\n",
        "        layers = [Transformer() for _ in range(depth)]\n",
        "        self.layers = nn.Sequential(*layers)\n",
        "\n",
        "    def forward(self , x):\n",
        "        x = self.layers(x)\n",
        "        return x"
      ],
      "execution_count": 112,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2mS_zIrwEQK4"
      },
      "source": [
        "x = torch.randn(2 , 197 , 768)\n",
        "te = Transformer_Encoder()\n",
        "z = te(x)\n",
        "z.shape"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vr__ky-zEYXI"
      },
      "source": [
        "class Classification(nn.Module):\n",
        "    def __init__(self , \n",
        "                 embed_dim = 768 , \n",
        "                 num_classes = 1000):\n",
        "        super(Classification , self).__init__()\n",
        "\n",
        "        self.linear1 = nn.Linear(embed_dim , num_classes)\n",
        "        self.norm = nn.LayerNorm(embed_dim)\n",
        "\n",
        "    def forward(self , x):\n",
        "        x = self.norm(x)\n",
        "        x = self.linear1(x)\n",
        "        x = reduce(x , 'b n e -> b e', reduction='mean')\n",
        "        return x"
      ],
      "execution_count": 116,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OXVe4jOLHtzw"
      },
      "source": [
        "x = torch.randn(2 , 197 , 768)\n",
        "classification = Classification()\n",
        "z = classification(x)\n",
        "z.shape"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "V8OUJ_KBH1Qw"
      },
      "source": [
        "class ViT(nn.Module):\n",
        "    def __init__(self , \n",
        "                 in_channels = 3 , \n",
        "                 patch_size = 16 , \n",
        "                 img_size = 224 , \n",
        "                 embed_dim = 768 , \n",
        "                 depth = 12 , \n",
        "                 num_classes = 1000):\n",
        "        super(ViT , self).__init__()\n",
        "\n",
        "        self.position = Patch_Embedding(\n",
        "            in_channels , \n",
        "            patch_size , \n",
        "            embed_dim , \n",
        "            img_size\n",
        "        )\n",
        "\n",
        "        self.transformer_encoder = Transformer_Encoder(\n",
        "            embed_dim , \n",
        "            depth\n",
        "        )\n",
        "\n",
        "        self.classifier = Classification(\n",
        "            embed_dim , \n",
        "            num_classes\n",
        "        )\n",
        "\n",
        "    def forward(self , x):\n",
        "        x = self.position(x)\n",
        "        x = self.transformer_encoder(x)\n",
        "        x = self.classifier(x)\n",
        "        return x"
      ],
      "execution_count": 119,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XlfUKAiSIEk5"
      },
      "source": [
        "x = torch.randn(2 , 3 , 224 , 224).to(device)\n",
        "vit = ViT().to(device)\n",
        "z = vit(x)\n",
        "z.shape"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8RVcO7fOI3Np"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}